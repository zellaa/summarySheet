\documentclass[a4paper,10pt]{article}
%\documentclass[a4paper,10pt,landscape]{article}
\usepackage[top=2.5cm,bottom=2.5cm,left=2.5cm,right=2.5cm,showframe]{geometry}
\usepackage{xcolor,fancyhdr}
\usepackage{tikz}
\usepackage{amsmath,amssymb,amsthm,amsfonts,physics}
\usepackage{stmaryrd}
\include{stylefile}

\usepackage{lineno}
\linenumbers
\setpagewiselinenumbers

%%%%%%%%%%%%%  PLEASE DO NOT EDIT ANY OF THE LINES ABOVE %%%%%%%%%%%%%%%
% Insert your text between "\begin{document}" and "\end{document}" below. 
% The total length of your summary notes should not exceed 2 sides of a
% single sheet of A4, with maximum 58 lines of text per page.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{cancel}
\DeclareMathOperator*{\argmin}{arg\,min}
\definecolor{fg}{RGB}{34,139,34}
\begin{document} \noindent
\textcolor{red}{NLA:}
\textbf{Golub} \textcolor{fg}{for $k=1:m,n$:} $u_k = (sgn(b_{k,k}) \norm{b_{k:m,k}}e_1 + b_{k:m,k}$); $u_k := \hat u_k$; $U_k := I-2u_k u_k^T$; $B_{k:m,k:n} := U_k B_{k:m,k:n}; U = [I_{k-1,k-1},0;0,U_k];$\textcolor{blue}{for $j=1:m,n-1$:} $v_k^T := sgn(b_{k,k+1}) \norm{b_{k,k+1:n}} e_1+ b_{k:m,k}$;$V_k := I - 2v_kv_k^T; B_{1:m,k+1:n} = B_{1:m,k+1:n} V_k$; $V = [I_{k,k},0;0,V_k]$\textcolor{blue}{endfor} \textcolor{fg}{endfor}; $2 \cdot (2mn^2 - 2 n^3/3)$
\textbf{Householder} \textcolor{blue}{for $k=[1,n]:$} $x=A_{k:m,k}; v_k = sgn(x) \norm{x}e_k+x;v_k= \frac{v_k}{\norm{v_k}} $ \textcolor{fg}{for $j=[k,n]$} $A_{k:m,j} = A_{k:m,j} - 2v_k\left[ v_k^* A_{k:m,j} \right]$ \textcolor{fg}{endfor} \textcolor{blue}{endfor}. $2mn^2 - \frac{2n^3}{3} $. 
\textbf{MG-S} $V=A;$\textcolor{blue}{for $i=[1,n]:$} $r_{ii}=\norm{v_i}; q_i = \frac{v_i}{r_{ii}}$;\textcolor{fg}{for $j=[i+1,n]$} $ v_j = v_j - ( q_i^T v_j )q_i; r_{ij} = q_i^Tv_j $ \textcolor{fg}{endfor} \textcolor{blue}{endfor}. $ 2mn^2$. 
\textbf{Arnoldi:} $q_1 := \hat b; q_{k+1} h_{k+1,k} = \textcolor{fg}{Aq_k - \sum_{i=1}^k q_i h_{ik}}$; $h_{ik} = q_i^T (A q_k)$; $h_{k+1,k} := \textcolor{fg}{\norm{ v}} \rightarrow AQ_k := Q_k H_k + q_{k+1} [0 \ldots h_{k+1,k}]$.
\textbf{Givens} $3mn^2$
\textbf{SVD:} $= \sum_i^{r:= \min{m,n}} u_i \sigma_i v_i^T$.
\textbf{C-F:} $\sigma_i(A) \sigma_n(B) \leq \sigma_i(AB) \leq \sigma_i(A) \sigma_1(B)$
\textbf{QR Algo:} $A_{k+1} = Q_k^TA_kQ_k \rightarrow A_{k+1} = \left( Q^{(k)} \right)^T A Q^{(k)}$.
Next $A^{k-1} = (Q_1 \ldots Q_{k-1}) (R_{k-1} \ldots R_1)$, so $A_k = Q_k R_k = (Q^{(k-1)})^TAQ^{(k-1)}$ so $Q^{(k-1)}A_k = AQ^{(k-1)}$. So $A^k = (A Q^{(k-1)})R^{(k-1)} = Q^{(k)}R^{(k)}$ as $A_k = Q_k R_k$.
\textbf{Krylov:} Usually want $x_k-x_0 \in \mathcal{K}_k$. Also note to show \textcolor{blue}{CG span properties} first show $\left\{ r_k \right\} \subset \left\{ p_k \right\}$ etc, then show $ \left\{ p_k \right\} $ LI.
\textbf{GMRES:} $\min \norm{AQ_k y -b}_2 \rightarrow \min \norm{H_k y - \norm{b}e_1}$. Bound $\norm{r_k} = \norm{ M p(\Lambda) M^{-1} r_0}$
\textbf{GMRES Conv:} If $x_k = p_{k-1}(A)b$ have $\min \norm{Ax_k - b} =  \min_{p(0)=1} \norm{A p_{k-1}(A)b-b} \leq k_2(A) \norm{p(\Lambda)b}$ with $p(0)=1$
\textbf{CG Bound:} With $c = x-x_0, c_k = x_k-x_0$ s.t. $r_k=A(c-c_k)$ we have $r_k^T v=0 \; \forall \; v \in \mathcal{K}_k$ so $v^tA(c-c_k)=0$, s.t. $y=c_k = \argmin \norm{c-y}_A$. WTS $e_k = e_0 p_k(A)$ with $p(0)=1$, and write $e_0 := \sum \gamma_i v_i$ with $Av_i = \lambda_i v_i \rightarrow \norm{e_k}_A = \min_{p_k,p(0)=1}\max \abs{p(\lambda_i} \norm{e_0}_A$
\textbf{CG Convergence:} $\norm{e_k}_A = \min_{p(0)=1} \norm{p_k(A)e_0}= \min_{p_k(A)} \max \abs{p_k(\lambda)} \norm{e_0} \rightarrow \leq 2\left( (\sqrt{k_2} -1) /(\sqrt{k_2} +1)\right)^k$; need $\alpha := 2(\lambda_1 + \lambda_2)$
\textbf{Cheb:} $T_k(x) = \frac{1}{2} (z^k + z^{-k}); 2xT_k = T_{k+1} + T_{k-1}$
\textbf{Cheb Shift:} Choose $p(x) = T_k(\left[ 2x-b-a \right]/\left[ b-a \right])/T_k\left( \left[ -b-a \right]/\left[ b-a \right] \right)$ s.t. $p(0)=1$.
Then $p \leq 1/ \abs{T_k\left( \left[ -b-a \right]/\left[ b-a \right] \right)} \leq 2\left( [\sqrt{\kappa}-1]/[\sqrt{\kappa} +1] \right)^k$
\textbf{CG Conditions:} To show $r_{k+1}^Tr_k = 0$ first show $p_k^TAp_k = p_k^TA r_k$ via $\beta$ then show $p_k ^T r_k = r_k^T r_k$ via $p_{k-1}^Tr_k=0$.
\textbf{Preconditioning:} For GMRES solve $MAx = Mb$ with $MA$ eigvals clustered far from 0, well conditioned. E.g. if $A=LU$ do $(LU)^{-1}$. For CG, $A=A^T$ difficult so want $M^T M \approx A^{-1}$.
So want $M^TAMy=M^Tb$, same properties.
\textbf{MP:} $\sigma(G) \in [\sqrt{m}- \sqrt{n},\sqrt{m}+\sqrt{n}] \rightarrow k_2 = O(1)$
\textbf{Sketch:} with $ GA \hat x= Gb$, and via $C-F$ $\norm {G [A,b] [v,-1]^T} \leq (s+\sqrt{n+1}) \norm{R [v,-1]^T}$,
similar for lower bound via MP 
$\rightarrow \norm{A \hat x-b}  \leq (\sqrt{s} + \sqrt{n+1})/(\sqrt{s} - \sqrt{n+1}) \norm{Ax-b}$
\textbf{Blend:} solve $\norm{(A\tilde R^{-1}) y-b}=0$ via CG;$k_2(A\tilde R^{-1}) = O(1)$ with $GA = \textcolor{blue}{\tilde Q\tilde R}$ \textcolor{fg}{PROOF:}
$A=QR; GA = GQR = \hat G R$. Let $\hat G = \hat Q \hat R$ so $G A = \hat Q \hat R R = \textcolor{blue}{\hat Q  (\hat R R)}\rightarrow \tilde R^{-1} = R^{-1} \hat R^{-1} \rightarrow k_2(A \tilde R^{-1}) = k_2(\hat R^{-1}) = O(1)$ by MP. $O(mn)$ to solve via normal
\textbf{HMT:} For $X = n\times r$ let $AX = QR$, then if $A=U_r \Sigma_r V^T_r$, span($Q$)=span($U_r$) so $\hat A = Q Q^T A$ is a rank $r$ approximant.
\textbf{HMT Proof:} Goal $ \norm{A- \hat A} = O(1) \norm{A-A_r}$. Have $(I-Q Q^T)AX=0$ so $A-\hat A = (A_n - QQ^T)A(I_n - XM^T)=0 \; \forall \; M^T$. Choose $M^T = (V^TX)^{\dagger}V^T$, $V \in n\times \hat r \leq r$. Let $XM^T=P$ s.t. $A(I-P)=A(I-VV^T)(I-P)$. 
So $ \norm{A-\hat A} = \norm{(I_m-QQ^T) U_A \Sigma_A \textcolor{blue}{[\tilde V^T_{\hat r}, \tilde V_{\hat r+1}^T]^T(I-VV^T)}(I-P) }$. 
If $V = \tilde V_r$ then $= \norm{(I_m-QQ^T) U_A \Sigma_A \textcolor{blue}{[0, \tilde V_{\hat r+1}]^T}(I-P) } \leq \norm{\Sigma_{\hat r+1}} \textcolor{blue}{\norm{I_n- XM^T}}$.
Now note $\textcolor{blue}{\norm{I_n- XM^T}} = \norm{I_n - X (\tilde V_r^TX)^\dagger \tilde V_r^T} \leq 1 + \norm{X} \norm{(\tilde V_r^T X)^\dagger}$.
Now $ \norm{X} \leq \sqrt{n} + \sqrt{r}$ by MP, and 
$ \norm{(\tilde V_r^T X)^\dagger} = \sigma_n(\tilde V_r^T X)^{-1} \leq (\sqrt{r} - \sqrt{\hat r})^{-1}$ by MP. 
So $ \norm{XM^T} \leq  \frac{\sqrt{n}+ \sqrt{r}}{\sqrt{r} - \sqrt{\hat r}} $.
\textbf{Bounds:} $\norm{ABB^{-1}} \geq \norm{AB} \norm{B^{-1}} \rightarrow \norm{A}/\norm{B^{-1}}\geq \norm{AB}$.
\textbf{Weyls:} $\sigma_i(A+B) = \sigma_i(A) + [-\norm{B},\norm{B}]$
\textbf{Rev $\Delta$ Ineq:} $\norm{A-B} \geq \abs{ \norm{A}-\norm{B} }$
\textbf{Courant Application:} $\sigma_i\left( [A_1;A_2] \right) \geq \max\left( \sigma_i (A_1), \sigma_i(A_2) \right)$
\textbf{Schur:} Take $A v_1 = \lambda_1 v_1$; construct $U_1 = [v_1, V_\perp] \rightarrow A U_1 = U_1 [e_1,X]$. Repeat.
\textbf{Conditioning} $\kappa_2(A) = \sigma_1 /\sigma_n = \norm{A}\norm{A^{-1}}$
\textbf{Similarity:} $A \rightarrow P^{-1}AP$, same $\lambda$.
\textbf{Pseud-Inv:} $A^\dagger  = V \Sigma^{-1} U^T = (A^T A)^{-1}A^T$. Else have $A^\dagger = A^T(AA^T)^{-1}$
\\ \textcolor{red}{CO:} 
\textbf{G-N:} $ \va*{x}_{k+1} = \va*{x}_k - \frac{\grad f_k}{J^T J} $, with $J:=$ Jacobian of $r(x)$
\textbf{Linesearch Convergence:} Show $x_{k+1} - x_* = \Psi(x_k)-x_* = \Psi(x_* + e_k) -x_*$ and taylor expand.
\textbf{SD:} $\norm{x_{k+1}-x_*} \leq \left( k_2(H)-1 \right)/(k_2(H)+1) \norm{x_k-x_*}$ with $H$ hessian. Also note with EXACT linesearch for quadratic, $H(x-x_*) = -s$.
\textbf{Rayleigh:} $ \frac{s^T H s}{ \norm{s}^2} \leq \norm{H}$
\textbf{bArm:} To show existence of $\alpha$, have $\phi(\alpha) = f(x_k + \alpha_k s_k), \psi(\alpha) = \phi(\alpha) - \phi (0)- \beta \alpha \phi'(0) \leq 0$, show $\psi'(0) = (1-\beta) \phi'(0) \leq 0 \rightarrow \psi(\alpha) \downarrow$ with $\alpha$.
\textbf{BFGS:} To show $H_{k+1} \geq 0$ nec. $\gamma^T \delta > 0$. Suff via $\gamma, \delta$ LI $\rightarrow $ use $\norm{\cdot}_H \rightarrow \gamma^T \delta > 0$.
\textbf{Quad Penalty Meth} With $y = -c/\sigma, \norm{\grad_\sigma \Phi} \leq \epsilon^k, \sigma^k \rightarrow 0,x\rightarrow x_*, \grad c(x_*)$ LI, then $y \rightarrow y_*$, $x\rightarrow KKT$, if $f,c \in C^1$. Also need $J_*^T$ full row rank.
\textcolor{fg}{PROOF:} If $y_* := J^\dagger_* \grad f_* \rightarrow \norm{y_k - y_*} = \norm{J^\dagger_* \grad f_* - y_k} \leq \cancel{\norm{J_k^{\dagger}\grad f_k - J^\dagger_* \grad f_*}} + \norm{J_k^\dagger \grad f_k - y_k}$.
Next $\norm{J_k^\dagger \grad f_k - y_k} \leq \norm{J^\dagger_k}\norm{\grad_\sigma \Phi} \rightarrow 0 $. 
Also, $\grad f_* - J^T_* y_* \rightarrow 0$, and $c_{k\rightarrow *} = - \sigma^{k\rightarrow *} y_{k \rightarrow *} = 0$ so $x_* \rightarrow KKT$
\textbf{Quad Pen. Meth Newt} Have $w = (J \Delta x + c)/\sigma$ so $[\grad^2 f, J^T;J,-\sigma I] [\Delta x, w]^T = -[\grad f,c]$
\textbf{Trust Region Radius:} $\rho_k := (f(x_k) - f(x_k+ s_k))/(f(x_k) - m_k(s_k))$
\textbf{TR-Method:} If $\rho \geq 0.9 $ then double radius, update step $x_{k+1} = x_k + s_k$. If $\rho \geq 0.1$ then same radius, update step. If $\rho$ small shrink radius, don't update step.
\textbf{Cauchy:} \textcolor{blue}{Is the point on gradient which minimises the quadratic model within TR}. Want $m_k(s_k) \leq m_k(s_{kc})$, where \textcolor{fg}{$s_{kc} := - \alpha_{kc} \grad f(x_k)$}, and \textcolor{fg}{$\alpha_{kc}:= \argmin m_k\left( \alpha \grad f(x_k) \right)$} subject to $\norm{\alpha \grad f} \leq \Delta$, i.e. $\alpha_{max} := \Delta/\norm{ \grad f}$.
\textbf{Calculation of Cauchy:}
We want to prove cauchy model decrease i.e. \textcolor{fg}{$ f(x_k) - m_k(s_k) \geq f(x_k) - m_k (s_{kc}) \geq 0.5 \norm{\grad f_k} \min\left\{ \Delta_k, \frac{\norm{\grad f_k}}{\norm{\grad ^2 f_k}}  \right\}$}.
First define $\Psi (\alpha) := m_k(- \alpha\grad f )$ s.t. $\Psi := f_k - \alpha \norm {f_k}^2 - 0.5 \alpha^2 H_k$, with $H_k := \left[ \grad f_k \right]^T \left[ \grad^2 f_k \right] \left[ \grad f_k \right]$. 
N.B. that \textcolor{blue}{$\alpha_{min}:= \frac{\norm{\grad f_k}^2}{H_k} $} if $H_k  >0$, from $\Psi'(0) <0$.
Now \textcolor{blue}{A: If $H_k \leq 0$} 
then we have $\Psi (\alpha) \leq f_k - \alpha \norm{\grad f_k}^2 \rightarrow \alpha_{kc} = \alpha_{max}$. So, we have $f_k - m_{s_k} \geq f_k - m_{s_{kc}} \geq \norm{\grad f_k} \Delta_k \geq 0.5 \norm{\grad f_k} \min \left\{ \Delta_k \right\}$.
Now \textcolor{blue}{B.i: If $H_k > 0 \rightarrow \alpha_{kc} =\alpha_{min}$}. 
Here $f_k - m_{s_{kc}} = \alpha_{kc} \norm{\grad f}^2 - 0.5 \alpha_{kc}^2 H_k = \frac{\norm{\grad f}^4}{2 H_k} \geq \frac{\norm{\grad f}}{2} \min\left\{ \frac{\norm{\grad f}}{\norm{\grad^2 f}}  \right\}$ via C-S.
Now \textcolor{blue}{B.ii: If $H_k > 0 \rightarrow \alpha_{kc} =\alpha_{max}$}. 
Here $\Delta / \norm{\grad f} \leq \norm{ \grad f}^2 /H_k \rightarrow \alpha_{kc} H_k \leq \norm{\grad f}^2$. So $f_k - m_{kc} = -\alpha_{kc} \norm{\grad f}^2 + \frac{\alpha_{kc}^2}{2} H_k \geq \frac{\norm{\grad f}^2}{2} \alpha_{kc} \geq 0.5 \norm{\grad f} \min\left\{ \Delta_k \right\}$
\textbf{TR-Global Convergence:} If $m_k(s_k) \leq m_k (s_{kc})$ then either $\exists k \geq 0$ s.t. $\grad f_k =0$ or $\lim \norm{\grad f} \rightarrow 0$. Further, require $f \in C^2$, bounded below and also $\grad f$ L-cont. 
\textcolor{blue}{PROOF:} Using def of $\rho$, $f_k - f_{k+1} \geq \frac{0.1}{2} \norm{\grad f_k} \min\left\{ \ldots \right\}$ from above. Let $\norm{ \grad ^2 f} := L$, and assuming $\norm{\grad f} \geq \epsilon$ we have $f_k - f_{k+1} \geq 0.05 \frac{c}{L} \epsilon^2$ assuming TR has a lower bound $c\epsilon/L$. Then sum over all successful jumps s.t. $f_0 - f_{lower} \geq \sum_{i\in \mathbb{S}} f_i - f_{i+1} \geq \abs{\mathbb{S}} \frac{0.05c\epsilon^2}{L} $
\textbf{Solving TR Prob:} Solve secular $ \norm{s}^{-1} - \Delta ^{-1}=0$.
\textbf{KKT Feasibility:} Need $s^T J \geq 0$, $J_E^Ts =0$, and $s^T \grad f <0$.
\textbf{KKT Conditions:} \textcolor{fg}{REMEMBER $c \geq 0$, $\lambda \geq 0$!}
\textbf{First Order KKT (Equality):} If we have $x_*$ local min, then let $x = x_* + \alpha s$. Then we have $c_i(x(\alpha)) \rightarrow 0 = c_i(x_*) + \alpha s^T J \rightarrow s^T J=0$. Further, we have $f(x) = f(x_*) + \alpha s^T \grad f \rightarrow \alpha s^T \grad f \geq 0$. Repeat for negative $\alpha$ s.t. $s^T \grad f =0$. By Rank-Nullity (assuming $J_E(x_*)$ full rank), we have $\grad f_* = J^T_* y + s_*$ for some $y_*$, which then implies (after $s^T$ from LHS) that $\norm{s_*}=0$, so $\grad f_* = J^T_* y_*$.
\textbf{KKT 2nd Order} If we have $\min f$ with $c(x) \geq 0$, $2^{nd}$ order conditions are that $s^T \grad ^2 \mathcal{L} s \geq 0$ for all $s \in \mathcal{A}$, with $\mathcal{A}$ defined s.t: \textcolor{fg}{EITHER} 
$s^T J_i =0 \; \forall \; i$ s.t. $\lambda_i > 0, c_i =0$, 
\textcolor{fg}{OR} 
$s^T J_i \geq 0 \; \forall \; i$ s.t. $\lambda_i = 0, c_i =0$, for $J,c,\lambda$ evaluated at $x_*$
For EQUALITY constraints instead need positive definite $\forall$ $s$ s.t. $J^T s =0$
\textbf{Convex Problems} $\hat x= KKT \Rightarrow \hat x = \argmin f(x)$. Proof via $f \geq f(\hat x) + \grad f^T(x-\hat x)$ so $f \geq f(\hat x) + \hat y^T A (x-\hat x) + \sum_{i\in I} \lambda_i J^T_i(x-\hat x)$. Choose $Ax=b$, and note that $c_i$ concave s.t. $\lambda_i J^T_i(\hat x) (x-\hat x) \geq \lambda_i (c_i(x) - c_i(\hat x)) = \lambda_i c_i (x) \geq 0 \rightarrow f(x) \geq f(\hat x)$.
\textbf{Log-Barrier Global Convergence:} (for $f-\sum \mu \log(c_i)$) With $f\in C^1, \lambda_{ik} = \frac{\mu_k}{c_{ik}}, \norm{\grad f_u (x_k)} \leq \epsilon_k, \mu_k \rightarrow 0, x_k \rightarrow x_*$. Also, $\grad c(x_*) LI \; \forall \; i \in \mathcal{A}$ (active constraints). Then $x_*$ KKT and $\lambda \rightarrow \lambda_*$.
PROOF:
Have $J_A^\dagger(x_*) = (J_A(x_*) J_A(x_*)^T)^{-1}J_A(x_*)$. Also, $c_A=0,c_I >0$. So $\lambda = \mu/c \rightarrow 0$ so $\lambda_I =0$ as $c_I>0$. 
Next $\norm{\grad f_k - J_{Ak}\lambda_{Ak} } \leq \norm{\grad f_k - J^T_k \lambda_k} + \norm{\lambda_I} M_1 = \norm{\grad f_{\mu k}}\rightarrow 0$. 
Now $ \norm{J_A^\dagger \grad f_k - \lambda_{kA}} \leq \norm{J_A^\dagger} \norm{\grad f_k - J_{Ak}^T \lambda_{Ak} }\rightarrow 0$. 
So with triangle ineq $ \norm{\lambda_{kA}- J_{Ak}^\dagger \grad f_k + J_{Ak}^\dagger \grad f_k - \lambda_{A*}} \rightarrow 0$, via cont. of $\grad f$ and $J^\dagger$. Thus $\lambda_{Ak} \rightarrow \lambda_{A*} \geq 0$. 
Combine s.t. $\grad f_k - J_{Ak}^T \lambda_{AK}$ with $k \rightarrow *$ so get KKT.
\textbf{Primal-Dual Newton:} Have $\grad f = J^T \lambda, C(x) \lambda = \mu e$ so $[\grad ^2 \mathcal{L}, -J^T;\Lambda J, C] [dx, d\lambda]^T = - [\grad f -J^T \lambda, C\lambda -\mu e]^T$ 
\textbf{Augmented Lagrangian:} Same result as QUAD PEN METH but $x\rightarrow x_*$ if $\sigma \rightarrow 0$ for bounded $u_k$ or $u_k$ to $y_*$ for bounded $\sigma_k$. Proof via $\norm{c_k} \leq \sigma_k \norm{y_k - y_*} + \sigma_k \norm{u_k - y_*}$. If $u_k$ bounded then $\rightarrow 0$ as $\sigma \rightarrow 0$, else trivially if $u_k \rightarrow y_*$ then to $0$.
\textbf{GLM Global Convergence:} With $f\in C^1, \grad f$ L-Cont, and $f$ bdd below, then $\grad f_l = 0$ or $\lim \min \left\{ \frac{\abs{\grad f^T s }}{ \norm{s}},\abs{s^T \grad f}  \right\}$ to $0$. In non-trivial case, via bArmijo, $f_k - f_{k+1} \geq - \beta \alpha_k  s_k^T \grad f_k  = \beta \alpha_k \abs{ s^T_k \grad f_k}$. Sum s.t. $f_0 - f_{k+1} \geq \beta \sum \alpha_k \abs{ s^T_k \grad f_k}$, so term in sum to $0$. For all $k$ successful we then have $\alpha_k \abs{s^T_k \grad f_k} \geq \frac{(1-\beta)\tau }{L } \left( \frac{\abs{s^T_k \grad f_k}}{ \norm{s_k} }  \right)^2 \geq 0$ so squared term to $0$. For unsuccesful steps $\alpha_k \geq \alpha_0$ so no norm term.
\textbf{Convergence Newton LSearch:} Need $f\in C^2$ then if $H_k$ (hessian) bdd above and below, so $\lambda_n \leq \lambda(H_k) \leq \lambda_1$. So $\abs{s^T_k \grad f_k} \geq \lambda_1^{-1} \norm{\grad f_k}^2$. Also $ \norm{s_k}^2 \leq \lambda_n^{-2} \norm{\grad f_k}^2$. 
Thus $\lim \min \left\{ \lambda_n \lambda_1^{-1} \norm{\grad f}, \lambda_1^{-1} \norm{\grad f}^2 \right\} \rightarrow 0$ from GLM Convergence Thm.
\end{document}
