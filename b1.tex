\documentclass[a4paper,10pt]{article}
%\documentclass[a4paper,10pt,landscape]{article}
\usepackage[top=2.5cm,bottom=2.5cm,left=2.5cm,right=2.5cm,showframe]{geometry}
\usepackage{xcolor,fancyhdr}
\usepackage{tikz}
\usepackage{amsmath,amssymb,amsthm,amsfonts,physics}
\usepackage{stmaryrd}
\include{stylefile}

\usepackage{lineno}
\linenumbers
\setpagewiselinenumbers

%%%%%%%%%%%%%  PLEASE DO NOT EDIT ANY OF THE LINES ABOVE %%%%%%%%%%%%%%%
% Insert your text between "\begin{document}" and "\end{document}" below. 
% The total length of your summary notes should not exceed 2 sides of a
% single sheet of A4, with maximum 58 lines of text per page.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\definecolor{fg}{RGB}{34,139,34}
\begin{document} \noindent
\textcolor{red}{NLA:} \textbf{Cholesky} For matrix $\left[ a_{11},w^*;w,K \right] = R_1^T \left[ I,0;0,K- \frac{w w^*}{a_{11}}  \right]\left[ \alpha, w^*/\alpha;0,I \right]  $ we have a decomp: \textcolor{blue}{for $k=[1,m-1]:$} \textcolor{fg}{for $j=[k+1,m]$} $R_{j,j:m} = R_{j,j:m} - \frac{R_{kj}}{R_{kk}} R_{k,j:m}$ \textcolor{fg}{endfor} $R_{k,k:m} = \frac{R_{k,k:m}}{\sqrt{ R_{kk} }} $ \textcolor{blue}{endfor}. $ \frac{m^3}{3} $. 
\textbf{Householder} \textcolor{blue}{for $k=[1,n]:$} $x=A_{k:m,k}; v_k = sgn(x) \norm{x}e_k+x;v_k= \frac{v_k}{\norm{v_k}} $ \textcolor{fg}{for $j=[k,n]$} $A_{k:m,j} = A_{k:m,j} - 2v_k\left[ v_k^* A_{k:m,j} \right]$ \textcolor{fg}{endfor} \textcolor{blue}{endfor}. $2mn^2 - \frac{2n^3}{3} $. 
\textbf{LU} $U=A,L=I$ \textcolor{blue}{for $k=[1,m-1]:$} \textcolor{fg}{for $j=[k+1,m]$} $L_{jk} = \frac{U_{jk}}{U_{kk}}; U_{j,k:m} = U_{j,k:m} - (\frac{U_{jk}}{U_{kk}}) U_{k,k:m}  $ \textcolor{fg}{endfor} \textcolor{blue}{endfor}. $ \frac{2m^3}{3} $. 
\textbf{MG-S} $V=A;$\textcolor{blue}{for $i=[1,n]:$} $r_{ii}=\norm{v_i}; q_i = \frac{v_i}{r_{ii}}$;\textcolor{fg}{for $j=[i+1,n]$} $ v_j = v_j - ( q_i^T v_j )q_i; r_{ij} = q_i^Tv_j $ \textcolor{fg}{endfor} \textcolor{blue}{endfor}. $ 2mn^2$. 
\textbf{G-S} $V=A;$\textcolor{blue}{for $i=[1,n]$}\textcolor{fg}{for $j=[1,i-1]$} $ r_{ji} =q_j^T a_i; v_i = v_i - r_{ji}q_j $ \textcolor{fg}{endfor} $r_{ii} = \norm{v_i}; q_i = v_i/r_{ii}$ \textcolor{blue}{endfor}. $ 2mn^2$?. 
\textbf{Givens} $3mn^2$
\textbf{SVD:} $= \sum_i^{r:= \min{m,n}} u_i \sigma_i v_i^T$.
\textbf{Bounds:} $\norm{ABB^{-1}} \geq \norm{AB} \norm{B^{-1}} \rightarrow \norm{A}/\norm{B^{-1}}\geq \norm{AB}$.
\textbf{Weyls:} $\sigma_i(A+B) = \sigma_i(A) + [-\norm{B},\norm{B}]$
\textbf{Norms:}$\norm{A}_F = \sqrt{\sum_i\left( \sigma_i \right)^2} =\sqrt{Tr\left( AA^T \right)}$, $\norm{A}_\infty=$ max row sum.
\textbf{Rev $\Delta$ Ineq:} $\norm{A-B} \geq \abs{ \norm{A}-\norm{B} }$
\textbf{Low-Rank:} For $A\in \mathbb{R}^{m\times n} \min \norm{A-B} = \norm{A-A_r}$. Proof via $B := B_1 B_2^T$ with $B_1 \in \mathbb{R}^{m\times r}$; $\exists W s.t. B_2^{T} W = 0$ with null($W$)$\geq n-r$. Then $\exists \: x_V,x_W s.t. V_{r+1}x_V = - W x_W$. So $\norm{A-B} = \norm{AW} \geq \norm{A V_{r+1} x_V} \geq \sigma_{r+1}$ For reverse $B:= A_r$
\textbf{Courant:} $\sigma_i = \max_{dim(S)=i}\left\{ \min_{x} \norm{Ax}/\norm{x} \right\}$. Proof via $V_i = [v_i \dots v_n]$, so dim($S$)+dim($V_i$) $=n+1$ so $\exists \: w\in S \cap V_i$. Then $\norm{Aw} \leq \sigma_i$. For reverse take $w=v_i$ when $S=[v_1\dots v_i]$
\textbf{Schur:} Take $A v_1 = \lambda_1 v_1$; construct $U_1 = [v_1, V_\perp] \rightarrow A U_1 = U_1 [e_1,X]$. Repeat.
\textbf{Back Subst:} For $Ux=y$ we have $x_{n-i} = \left( y_{n-i} - \sum_{n-i+1}^n u_{n-i,j}x_j \right)/u_{n-i,n-i}; O(i)$ per iteration so $O(n^2)$ total.
\textbf{Backwards Stable:} When $\hat f(x) = f (x + \Delta x)$ with $\norm{\Delta x}/\norm{x} \leq O(\varepsilon)$
\textbf{Conditioning} $\kappa_2(A) = \sigma_1 /\sigma_n = \norm{A}\norm{A^{-1}}$
\textbf{Similarity:} $A \rightarrow P^{-1}AP$, same $\lambda$.
\textbf{Elementary L:} Define via $L_i(m) = I - me_i^T$
\\ \textcolor{red}{NPDE:} \textbf{Def'n:} With $u_{tt}-c^2 u_{xx} = f$ have $\Delta x =(b-a)/J, \Delta t = T/M, x_j = a+j \Delta x, t = m \Delta t$. I.C: $U^0_j = u_0(x_j), U_j^1 = U_j^0+ u_1(x_j) \Delta t, U_0^m = U_J^m = 0$
\textbf{Hyp Impl:} $\left( A-B,A \right) = \frac{1}{2}( \norm{A}^2-\norm{B}^2) + \frac{1}{2} \norm{A-B}^2$ with $\textcolor{blue}{A:= U^{m+1}-U^m}, \textcolor{fg}{B:= U^m - U^{m-1}}$ (T);$(-D_x^+D_x^-U^{m+1},U^{m+1}-U^m) = ( D_x^-U^{m+1} - D_x^- U^m,D_x^-U^{m+1})$ (X). Then $\frac{1}{2 \Delta t^{2}} ( \norm{U^{m+1}-U^m}^2 - \norm{U^m- U^{m-1}}^2 ) + \frac{\Delta t^2 }{ 2 \Delta t^2} \norm{ U^{m+1}-2U^m+U^{m-1} }^2 + \frac{c^2}{2} \left( \norm{ D_x^- U^{m+1} }^2 - \norm{D_x^-U^{m}}^2 \right) + \frac{c^2 \Delta t^2}{ 2 \Delta t^2} \norm{D_x^- (U^{m+1} - U^m)}^2 =
\textcolor{fg}{(f, U^{m+1} - U^m)} $. Then
\textcolor{blue}{$M^2(U^m) :=\norm{ \frac{U^m- U^{m-1}}{\Delta t}}^2+ c^2 \norm{D_x^- U^{m+1}}^2 $}. Write \textcolor{fg}{green} as $\leq \norm{f}\norm{U^{m+1}-U^m} = \sqrt{ \Delta t T} \norm{f} \sqrt{\frac{\Delta t}{T}} \norm{ \frac{U^{m+1}-U^m}{\Delta t}  } \leq \textcolor{fg}{\frac{\Delta t T}{2} \norm{f}^2 + \frac{\Delta t}{2T } \norm{ \frac{U^{m+1}- U^m}{\Delta t} }^2}$. Then $(1 - \frac{\Delta t}{T} ) M^2(U^m) \leq M^2(U^{m-1}) + \Delta t T \norm{f}^2 \rightarrow M^2 (U^m) \leq (1+ \frac{2 \Delta t}{T} )M^2 (U^{m-1})+ 2 \Delta t T \norm{f}^2$. Use \textcolor{blue}{$a_m \leq \alpha^m a_0 + \sum_{k=1}^m \alpha^{m-k}b_k$} so $M^2 \leq e^2 M^2(U^0) + 2 e^2 T \sum_{k=1}^m \Delta t \norm{f}^2$
\textbf{Hyp Expl:} 1st rewrite in terms of $D_t^{+-} (\Delta t)^{-2}U_j^m + \frac{c^2 (\Delta t)^2}{4} D_x^{+-}((\Delta t)^{-2}D_t^{+-}U_j^m) -\\(c^2/4) D_x^{+-}\left( U_j^{m+1}+2U_j^m+U_j^{m-1} \right)$. Then use $\color{blue}{ \left( D(A-B),A+B \right)} = (DA,A)-(DB,B);\\
\left( D(A+B),A-B \right) = (DA,A)-(DB,B) $ by multiplying by $U^{m+1} - U^{m-1}$. Finally WTS $\norm{V_m}^2- \frac{c^2 \left( \Delta t \right)^2}{4} \norm{D_x^- V^m}^2 \geq 0$. Done by noticing: $\norm{D_x^- V^m}^2 = \sum_i^J \Delta x | D_x^- V_j^m|^2 = \frac{1}{\Delta x}  \sum_i^J \left( V_j^m - V_{j-1}^{m} \right)^2 \leq 2/\Delta x \sum_i^J (V_j^m)^2 + (V_{j-1}^{m})^2 = 4/\Delta x^2 \sum_i^{J-1} \Delta x \left( V_j^m \right)^2$. Eventually show $N^2(U^m) := \\ \left(  \left( I + \frac{c^2 \Delta t^2 }{2} D_x^{+-} \right) \frac{U^{m+1}- U^m}{\Delta t} , \frac{U^{m+1} - U^m}{\Delta t}  \right) + c^2 \norm{ D_x^- \frac{U^{m+1} +U^m }{2} }^2 \rightarrow N^2(U^m) =  N^2(U^{m-1})+(f,U^{m+1}-U^m)$
\textbf{Max Principle:} For $- \Delta u = f \leq 0 \rightarrow \max u \in \partial D$. First show contradiction assuming $LU = f < 0$, then try some auxillary function $\psi = U + \alpha\left( T_{\max} \right) g\left( x_i,y_i \right)$ s.t. $L\psi < 0$ so $\max \psi = \max_{\in \partial D} \psi$. Gets $\max e_{i,j}$; change to $-\alpha$ for $\min e_{i,j}$.
\textbf{P-F Ineq:} $\norm{V}^2_h \leq c_\star ||D_x^-V]|^2$. \textcolor{fg}{For 2D:} $ | V_j^m| = |\sum_{\alpha=1}^{j} h(D_x^- V_\alpha^m)|^2 \leq jh \sum_{\alpha=1}^{N-1} h | D_x^- V_\alpha^m|^2 \rightarrow \norm{V}_h^2 = \textcolor{blue}{\sum_{j=1}^{N-1}h |V_j^m|^2} \leq \sum_{j=1}^{N-1} jh^2 \sum_{\alpha=1}^{N-1}h|D_x^- V_\alpha^m|^2 \leq \textcolor{blue}{\frac{1}{2} \sum_{j=1}^N h|D_x^-V_j^m|^2}$. Use \textcolor{blue}{blue} and add for $x,y$ for $c_\star = 0.25$.
\textbf{Weak Deriv:} $w$ is a weak derivative of $u$ if $\int dx \: w v = \left( -1 \right)^{\abs{\alpha}} \int dx \: u (D^\alpha v)$
\textbf{Parseval:} $\textcolor{blue}{\int dk \: \hat u(k) v(k)} = \int dk \:v(k) \left( \int dx \: u(x) e^{-ixk} \right) = \int dx\:  u(x) \left( \int dk \: v(k) e^{-ixk} \right) = \textcolor{fg}{\int dx \: u(x) \hat v(x)}$. Now $v(k) := \textcolor{blue}{\overline{\hat u(k)}}  = \overline{F\left[ u(k) \right]} = \overline{ \int dk \: u(k) e^{-ixk} } = \\\int dk \: \overline{u(k)} e^{ixk} = 2\pi F^{-1}\left[ \overline{u(k)} \right] \Rightarrow \textcolor{fg}{\hat v(x) = 2 \pi \overline{u(x)}}$ 
\textbf{Iterative:} If $U^{j+1} = U^j- \tau \left( AU^j - F \right) \rightarrow U-U^j  = \left( I-\tau A \right)^j \left( U-U^0 \right)$ so $\norm{U-U^j} \leq \textcolor{blue}{\norm{I-\tau A}}^j\norm{U-U^0}$. $\textcolor{blue}{\norm{I-\tau A}} = \sigma_1 = \abs{\lambda_1}$ as symmetric. If $\lambda \in [\alpha, \beta]$ then $\lambda_1 \leq \max{ \left\{ \abs{1-\tau \alpha},\abs{1-\tau \beta} \right\} }$. Attained when $\tau = 2/(\alpha + \beta) \rightarrow \lambda_1  = \frac{\beta - \alpha}{ \beta + \alpha} $. For $-u''+cu=f$ we have $\lambda_k = c+ \frac{4}{h^2} \sin^2\left( \frac{k \pi h}{2}  \right)$. Lower bound via noting $\sin(y) \geq \frac{2 \sqrt 2}{\pi} y$ at $y= \frac{\pi}{4} \rightarrow \lambda_k \geq c+ 8$
\textbf{Errors:} $(AV,V)_h \geq ||D_x^{-}V]|^2_h$ \& PF Ineq $\rightarrow (AV,V)_h \geq \norm{V}_h^2/c_\star$. Then $(AV,V)_h\left( 1+c_\star \right) \geq \norm{V}_{1,h}^2 \rightarrow (AV,V)_h \geq c_0 \norm{V}_{1,h}^2$. Now $c_0 \norm{V}_{1,h}^2 \leq (AV,V)_h \leq \norm{f}_h \norm{V}_h \leq \norm{f}_h \norm{V}_{1,h} \rightarrow \textcolor{blue}{\norm{V}_{1,h} \leq \norm{f}_h/c_0}$. (Use $f:= AV \rightarrow \textcolor{blue}{ \norm{e}_{1,h} \leq \norm{T}_h /c_0}$).
\textbf{Scheme:} For e.g. on $(0,1)^2$ write $-\Delta u +u =-1$ and $u|_{\partial D} = b$, with $x_j, t_m$, we have scheme for $1 \leq j \leq J-1, 1\leq m \leq M-1$, and initial conditions $U_{j,0} = U_{j,N} = b $ for $ 0 \leq j \leq N$, and $U_{0,m} = U_{J,m} = b$ for $1 \leq m \leq M-1$. \textcolor{blue}{Might need to define scheme for $m=0$ if e.g. $\theta$ scheme with no $U_{i,j-1}$ terms such as $u_t = u_{xx}$}
\textbf{Non Uniform:} We have $h_{i+1} := x_{i+1} - x_i, h_i := x_i - x_{i-1} \rightarrow \hbar_i = \frac{1}{2} \left( h_{i+1} + h_i \right)$ so $D_x^+D_x^-U_j^m = \frac{1}{\hbar_i} \left( [U_{j+1} - U_j]/h_{i+1} - [U_j - U_{j-1}]/h_i \right)$. 
\textbf{L$_2$ F'n:} We approximate $f_{i,j} \rightarrow \frac{1}{h^2}  \int_{K_{i,j}} f$ where $K_{i,j} = [x_i \pm 1/2, y_i \pm 1/2]$. \textcolor{blue}{For errors:} NB that $Au-AU =  -D_x^+D_x^- u -D_y^+ D_y^- u + cu - T(\Delta u + cu)$. NB $Tu_{xx} = D_x^+ \frac{1}{h} \int u_x (x_i - \frac{h}{2} )dy := D_x^+ \alpha_x$ so $Ae_{i,j} = D_x^+ \phi_1 + D_x^- \phi_2 + \psi$, with $\phi_1:= \alpha_x - D_x^- u, \psi := cu - Tcu$. Now NB $c_0 \norm{e}_{1,h}^2 \leq (Ae,e)$. Bound $(D_x^+ \phi_1,e)$ via $\leq || \phi_1 ]|_x ||D_x^- e]|_h$ so $c_0 \norm{e}_{1,h}^2 = (|| \phi_1 ]|^2_x +|| \phi_2 ]|^2_y+ \norm{\psi}_h^2) \norm{e}_{1,h}$
\textbf{L-Bounds:} $ \frac{\abs{f(u)-f(v)}}{\abs{u-v}} \leq \abs{f'}$
\textbf{Hyperbolic Signs} For $u_t + au_x$ when using $[a]_{\pm}$ we write $D_t^- U_j^m + [a]_+ D_x^-U_{j}^m + [a]_-D_x^+ U_j^m$. Eventually get $U_j^{m+1} = \left( 1-\frac{\abs{a} \Delta T}{\Delta x} \right)U_j^m + \frac{[a]_+ \Delta t}{\Delta x} U_{j-1}^m- \frac{[a]_-\Delta t}{\Delta x} U_{j+1}^m$. Then via CFL assumption$ \frac{a\left( \norm{U^0}_\infty  \right)\Delta t}{\Delta x} \leq 1 \rightarrow |a(U)| \leq a(|U|) \leq a(\norm{U}_\infty)$ so $\norm{U^{m+1}}_\infty \leq \norm{U^0}_\infty$
\textbf{Summation by Parts:} We have $(-D_x^+ D_x^- U,U)  = - \sum_{i=1}^{N-1} h(D_x^+D_x^- U_i)U_i = - \sum_{i=1}^{N-1} \frac{U_{i+1}-U_i }{h} U_i + \sum_{i=1}^{N-1} \frac{U_i-U_{i-1}}{h} U_i = -\textcolor{blue}{ \sum_{i=2}^{N} \frac{U_i-U_{i-1}}{h} U_{i-1}  } + \sum_{i=1}^{N-1} \frac{U_i-U_{i-1}}{h} U_{i-1} = -\textcolor{fg}{ \sum_{i=1}^{N} \frac{U_i-U_{i-1}}{h} U_{i-1}  } + \textcolor{fg}{\sum_{i=1}^{N} \frac{U_i-U_{i-1}}{h} U_{i-1}} = \sum_{i=1}^N h |D_x^- U_i|^2 \\= ||D_x^-U]|^2_h$, where \textcolor{blue}{blue} from shift of index and \textcolor{fg}{green} from $U_0=U_N=0$.
\end{document}
